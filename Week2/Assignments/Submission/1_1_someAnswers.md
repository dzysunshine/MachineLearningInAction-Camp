### (1)立方体的太阳  
决策树分类算法原理：
找到决定性的特征，划分出最好的结果，原始数据集被划分为几个数据子集，这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则无需进一步对数据集进行分。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。划分数据子集的算法和划分原始数据集的方法相同，直到所有具有相同类型的数据在一个数据子集内。  

### (2)蓝石头河  
分类决策树：在一个数据集中找到一个最优特征，然后从这个特征的选值中找一个最优候选值，根据这个最优候选值将数据集分为两个子数据集。然后递归上述操作，直到满足指定条件为止。   

1. 最优特征：获得信息增益最高的特征；  
2. 最优候选值：选择最优候选值将多分类转化为二分类；    
3. 递归的结束条件：根据实际问题来选择，例如，叶节点样本数、最大树深度、分裂次数、最大特征数等。  

### (3)Sam   
决策树是一棵二叉或多叉树，它对数据的属性进行判断，得到分类或回归结果。预测时，在树的内部节点处用某一属性值（特征向量的某一分量）进行判断，根据判断结果决定进入哪个分支节点，直到到达叶子节点处，得到分类或回归结果。这是一种基于if-then-else规则的有监督学习算法，决策树的这些规则通过训练得到，而不是人工制定的。

### (4)萌萌  
（1）找到划分数据的特征，作为决策点  
（2）利用找到的特征对数据进行划分成n个数据子集。    

（3）如果同一个子集中的数据属于同一类型就不再划分，如果不属于同一类型，继续利用特征进行划分。  
（4）指导每一个子集的数据属于同一类型停止划分。  



### (5)九桢望乡 
决策树算法原理
基于树结构来进行决策，基于特征对实例进行分类的过程。一棵树上包含一个根节点，若干内部节点（特征或属性）和叶节点（类）与有向边组成，将实例从根逐步分配到叶节点。（递归）可以认为是if-then规则的集合。学习时，根据损失函数最小化原则建立模型，该模型需要注意特征的选取，防止过拟合问题。

### (6)ZengTudou 
算法定义：
   决策树是一种常用的机器学习算法，常用于分类；顾名思义，决策树是基于树结构来进行决策的。
一般情况下，一棵决策树包含一个根节点、若干个内部节点和若干个叶节点；叶节点对应于决策结果，其他每个节点对应一个属性测试；每个节点包含的样本集合根据属性测试的结果被划分到子节点中；根节点中包含所有的样本集合。
算法优点：

1、决策树算法的计算结果易于理解，这是由于树形结构使得数据条理清晰；  
2、可以处理不相关的特征数据。
 算法缺点：决策树算法容易导致过拟合问题的出现，虽然可以通过预剪枝或者后剪枝处理来减缓过拟合的影响，不过这两种方式仍然有一定的局限性，主要表现在：(1)预剪枝处理，可能将一些在后面出现好的结果的分支给删除掉了；(2)后剪枝处理，需要整个树结构计算完成后进行剪枝处理，并且需要自底向上对每一个非叶节点进行逐一考察，会导致训练开销时间比未进行剪枝处理增大很多。  

### (7)大秋 
决策树的原理，利用信息分类算法，根据数据的某个特性对数据进行分类，构造决策树，用熵的变化来衡量分类的增益。之后就可以利用构造好的决策树对未知数据进行分类。它的优点是决策树只需要构造一次，后续的数据分类过程非常快速。缺点是构造决策树很耗时过。



### (8)周星星 
决策树(decision tree)是一种基本的分类与回归方法。决策树的长方形代表判断模块(decision block)，
椭圆形成代表终止模块(terminating block)，表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作为分支(branch)，它可以达到另一个判断模块或者终止模块。分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。
结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。

### (9)慕凝   
决策树分类算法原理:  
从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配其子结点，这时，每一个子结点对应着该特征的一个取值，如此递归地对实例进行测试并分配，直至到达叶结点，最后将实例分到叶结点的类中。由决策树的根结点到叶结点的每一条路径构建一条规则;路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论，决策树的路径具有一个重要性质:互斥且完备，也就是说每一个实例都被一条路径或一条规则有所覆盖，而且只被一条路径或一条规则所覆盖。  
优点:  

1 可以处理多维度输出的分类问题   

2 基本不需要预处理 不需要提前归一化  处理缺失值    

3 对异常点的容错性能力好   
缺点:  

1  非常容易过拟合  泛化能力不强   

2  会因为样本发生一点点改动  就导致树结构的剧烈改变  



### (10)太阳老公 
决策树是通过一系列规则对数据进行分类的过程。在样本中有一些特征在分类时起到决定性的作用，
决策树就是找到这些具有决定性作用的特征，根据其决定性的影响来构造一个倒立的树，将决定性最大的那个作为根节点，然后递归找到各分支下数据集中次大的决定特征，直至数据集中所有数据都属于同一类。
决策树的优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不想干特征数据；
缺点：会产生过度匹配问题
适用数据类型：数值型和标称型



### (11)zzzzzying9... 
决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的建立。开始，构建根节点，将所有训练数据都放在根节点，选择一个最优特征，按照这一特征将训练数据分成子集，使得各个子集有一个当前条件下的最好的分类。如果这些子集已能够被基本正确分类，那么构建叶节点，并将这些子集分到对应的叶节点中；如果还有子集不能被正确分类，那么对于这些子集选择新的最优特征，继续对其分割，构建相应的节点。如此递归下去，直至所有训练子集被基本正确分类或者没有合适的特征为止。



### (12)PoleToWin 
决策树根据一步步地属性分类可以将整个特征空间进行划分，从而区别出不同的分类样本决策树是附加概率结果的一个树状的决策图，是直观的运用统计概率分析的图法。机器学习中决策树是一个预测模型，它表示对象属性和对象值之间的一种映射，树中的每一个节点表示对象属性的判断条件，其分支表示符合节点条件的对象。树的叶子节点表示对象所属的预测结果。
1）树以代表训练样本的单个结点开始。
2）如果样本都在同一个类．则该结点成为树叶，并用该类标记。
3）否则，算法选择最有分类能力的属性作为决策树的当前结点。
4）根据当前决策结点属性取值的不同，将训练样本数据集tlI分为若干子集，每个取值形成一个分枝，有几个取值形成几个分枝。匀针对上一步得到的一个子集，重复进行先前步骤，递4'I形成每个划分样本上的决策树。一旦一个属性出现在一个结点上，就不必在该结点的任何后代考虑它。
5）递归划分步骤仅当下列条件之一成立时停止：
①给定结点的所有样本属于同一类。
②没有剩余属性可以用来进一步划分样本．在这种情况下．使用多数表决，将给定的结点转换成树叶，并以样本中元组个数最多的类别作为类别标记，同时也可以存放该结点样本的类别分布。

③如果某一分枝tc，没有满足该分支中已有分类的样本，则以样本的多数类创建一个树叶。





### (13)谁家那小谁  
首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。
决策树是通过一系列规则对数据进行分类的过程。决策树分为分类树和回归树两种，分类树对离散变量做决策树，回归树对连续变量做决策树。
一棵决策树的生成过程主要分为以下3个部分:
特征选择：特征选择是指从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准标准，从而衍生出不同的决策树算法。
决策树生成： 根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。 树结构来说，递归结构是最容易理解的方式。
剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。



### (14)soya 
决策树分类算法（ID3算法）的原理：
1.收集数据集
2.计算经验熵H（D）和经验条件熵H（D|A），得到每个特征的信息增益值g（D,A）
3.比较每个特征的信息增益值，信息增益值最大的特征表示此特征在划分数据分类上起到的作用最大
4.通过此特征值划分原数据集为几个数据子集
5.若某个分支下的数据子集已经都属于同一类型，或者信息增益值足够小，则无需再对这类数据进行分割
6.若某个分支下的数据子集不属于同一类型，说明还需继续划分，对此分支下的子集重复2-4步的步骤，直到所有具有相同类型的数据均在一个数据子集内或者子节点内的信息增益值足够小



### (15)枫奇灵 
决策树是一种分类回归的方法，包括三个主要的步骤：特征选择、决策树生成、决策树修剪 

1、特征选择：在于选取对训练数据具有分类能力的特征，
该特征可以将数据分为两类 根据：选择信息增益大的那个特征进行分类

 2、决策树生成 2.1 ID3：在决策树各个节点上应用信息增益准则来选择特征，
递归的构建决策树。 步骤： （1）从根节点开始，对结点计算所有可能的特征的信息增益 （2）选择信息增益最大的那个特征作为节点的特征，由该节点的不同取值建立子节点 （3）对子节点递归调用以上方法，构建决策树 （4）直到所有特征的信息增益都很小或没有特征可以选择为止，最后得到一个决策树 该方法只有生成决策树，容易过拟合。 2.2 C4.5：与ID3相似，用信息增益比来选择特征。

 3、决策树的剪枝：针对过拟合问题，考虑决策树的复杂度，对决策树进行简化。利用损失函数极小化（正则化的极大似然估计） （1）计算每个点的经验熵 （2）递归的从树的叶节点向上回缩。假设回缩前后分别为Tb和Ta，其对应损失函数为C(Tb)和C(Ta)，若C(Tb)>C(Ta),则进行剪枝，将父节点变为新的叶节点。



### (16)云之影 
决策树分类算法原理：通过计算集合分类前的信息无序度、以及分别依据每个特征分类后，对单个特征各子集的信息无序度求和，比较分类前与分类后的信息增益，每次依据信息增益最大对应的特征进行分类，对剩下的子集做递归操作，直到分类子集内标签一致，或子集内只包含一个特征，分类停止。



### (17)车车  
决策树算法是一种逼近离散函数值的方法。它是一种典型的分类方法，首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。本质上决策树是通过一系列规则对数据进行分类的过程。
决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。
优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
缺点：可能会产生过度匹配的问题。